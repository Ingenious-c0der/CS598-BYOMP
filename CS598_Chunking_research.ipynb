{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BYOMP-598.png](BYOMP-598.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c-U05kEA6rGm",
    "outputId": "84ae2268-b455-4c1f-81d7-c624bec42689"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install --quiet transformers datasets langchain sentence-transformers faiss-cpu torch numpy pandas matplotlib tqdm chromadb accelerate langchain_experimental langchain_openai langchain_cohere \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eQ1-00T-Knrv"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import faiss\n",
    "from hashlib import sha256\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    TokenTextSplitter\n",
    ")\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lJQLmqv_DtRg"
   },
   "outputs": [],
   "source": [
    "def load_quality_github_dataset():\n",
    "    splits = {\n",
    "        \"train\": \"train.jsonl\",\n",
    "    }\n",
    "    dataset = {}\n",
    "    for split, filename in splits.items():\n",
    "        with open(f\"{filename}\", \"r\", encoding=\"utf-8\") as f:\n",
    "            dataset[split] = [json.loads(line) for line in f]\n",
    "    return dataset\n",
    "\n",
    "def flatten_quality_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Flatten the QuALITY dataset to create one entry per question\n",
    "    \"\"\"\n",
    "    flattened_dataset = {}\n",
    "\n",
    "    for split_name, samples in dataset.items():\n",
    "        flattened_samples = []\n",
    "        if split_name == \"test\":\n",
    "            continue\n",
    "        for article_data in samples:\n",
    "            article = article_data[\"article\"]\n",
    "\n",
    "            for question_data in article_data[\"questions\"]:\n",
    "\n",
    "                # Convert 1-indexed gold_label to 0-indexed\n",
    "\n",
    "                gold_label_idx = question_data[\"gold_label\"] - 1\n",
    "\n",
    "                gold_label_letter = chr(65 + gold_label_idx)  # Convert to A, B, C, D\n",
    "\n",
    "                flattened_samples.append({\n",
    "                    \"article\": article,\n",
    "                    \"question\": question_data[\"question\"],\n",
    "                    \"options\": question_data[\"options\"],\n",
    "                    \"gold_label\": gold_label_letter,  # Store as A, B, C, D\n",
    "                    \"article_id\": article_data[\"article_id\"],\n",
    "                    \"title\": article_data.get(\"title\", \"\")\n",
    "                })\n",
    "\n",
    "        flattened_dataset[split_name] = flattened_samples\n",
    "\n",
    "    return flattened_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "def load_wikiqa_dataset():\n",
    "    dataset = load_dataset(\"wiki_qa\")\n",
    "    \n",
    "    # WikiQA has different structure - we need to adapt it to our format\n",
    "    formatted_data = {\"train\": [], \"validation\": [], \"test\": []}\n",
    "    \n",
    "    for split in [\"train\", \"validation\", \"test\"]:\n",
    "        for example in dataset[split]:\n",
    "            # Create fake \"article\" from question and candidate sentences\n",
    "            context = f\"Question: {example['question']}\\n\" + \"\\n\".join(example['candidate_sentences'])\n",
    "            \n",
    "            formatted_data[split].append({\n",
    "                \"article\": context,\n",
    "                \"question\": example['question'],\n",
    "                \"options\": example['candidate_sentences'],\n",
    "                \"gold_label\": chr(65 + example['label']) if 'label' in example else None,\n",
    "                \"article_id\": example['question_id'],\n",
    "                \"title\": \"\"\n",
    "            })\n",
    "    \n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datasets import load_dataset\n",
    "\n",
    "def load_squad():\n",
    "    \"\"\"Load and preprocess SQuAD v1.1 dataset\"\"\"\n",
    "    dataset = load_dataset(\"squad\")\n",
    "    \n",
    "    formatted_data = {\"train\": []}\n",
    "    \n",
    "    for split in [\"train\"]:\n",
    "        for example in dataset[split]:\n",
    "            # SQuAD is extractive QA - create multiple choice format\n",
    "            context = example['context']\n",
    "            question = example['question']\n",
    "            \n",
    "            # Get the correct answer\n",
    "            answer_text = example['answers']['text'][0]  # Correct Answer text\n",
    "            \n",
    "            # Generate distractors from context\n",
    "            sentences = [s.strip() for s in context.split('.') if s.strip()]\n",
    "            # Filter out sentences containing the answer to avoid confusion\n",
    "            distractor_candidates = [s for s in sentences if answer_text not in s]\n",
    "            \n",
    "            # If we don't have enough distractors, create some\n",
    "            if len(distractor_candidates) < 3:\n",
    "                while len(distractor_candidates) < 3:\n",
    "                    distractor_candidates.append(f\"Incorrect option {len(distractor_candidates)+1}\")\n",
    "            \n",
    "            # Sample 3 distractors\n",
    "            distractors = random.sample(distractor_candidates, 3)\n",
    "            \n",
    "            # Randomly place the correct answer among options\n",
    "            correct_option_index = random.randint(0, 3)\n",
    "            options = []\n",
    "            \n",
    "            for i in range(4):\n",
    "                if i == correct_option_index:\n",
    "                    options.append(answer_text)\n",
    "                else:\n",
    "                    options.append(distractors.pop(0))\n",
    "            \n",
    "            # Convert index to letter (0->A, 1->B, etc.)\n",
    "            correct_option_label = chr(65 + correct_option_index)  # A, B, C, or D\n",
    "            \n",
    "            formatted_data[split].append({\n",
    "                \"article\": context,\n",
    "                \"question\": question,\n",
    "                \"options\": options,\n",
    "                \"gold_label\": correct_option_label,\n",
    "                \"article_id\": example['id'],\n",
    "                \"title\": example['title']\n",
    "            })\n",
    "    \n",
    "    return formatted_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load natural questions dataset\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_natural_questions():\n",
    "    \"\"\"Load and preprocess 2000 examples from Natural Questions into MCQ format.\"\"\"\n",
    "    input_file = \"sample_2000_rows.jsonl\"\n",
    "    formatted_data = {\"train\": []}\n",
    "    df = pd.read_json(input_file, lines=True)\n",
    "    print(len(df))\n",
    "    totalcount = 0\n",
    "    for i in range(len(df)): \n",
    "        data = df.loc[i]\n",
    "        context = data['document_text']\n",
    "        question = data['question_text']\n",
    "        answer_text = \"\"\n",
    "        short_answers = data['annotations']\n",
    "        if short_answers[0]['short_answers'] is not None:\n",
    "        \n",
    "            if not short_answers[0]['short_answers']:\n",
    "                totalcount +=1\n",
    "                continue\n",
    "            \n",
    "            si = short_answers[0]['short_answers'][0]['start_token']\n",
    "            ei = short_answers[0]['short_answers'][0]['end_token']\n",
    "            tokens = context.split(' ')\n",
    "            answer = tokens [si:ei]\n",
    "            answer_text = \" \".join(answer)\n",
    "    \n",
    "        sentences = [s.strip() for s in context.split('.') if s.strip()]\n",
    "        \n",
    "        distractor_candidates = [s for s in sentences if answer_text not in s]\n",
    "        \n",
    "        if len(distractor_candidates) < 3:\n",
    "            while len(distractor_candidates) < 3:\n",
    "                distractor_candidates.append(f\"Incorrect option {len(distractor_candidates)+1}\")\n",
    "        \n",
    "        # Sample 3 distractors\n",
    "        distractors = random.sample(distractor_candidates, 3)\n",
    "        \n",
    "        # Randomly place the correct answer among options\n",
    "        correct_option_index = random.randint(0, 3)\n",
    "        options = []\n",
    "        \n",
    "        for j in range(4):\n",
    "            if j == correct_option_index:\n",
    "                options.append(answer_text)\n",
    "            else:\n",
    "                options.append(distractors.pop(0))\n",
    "        \n",
    "        # Convert index to letter (0->A, 1->B, etc.)\n",
    "        correct_option_label = chr(65 + correct_option_index)  # A, B, C, or D\n",
    "        \n",
    "        # Prepare the output\n",
    "        output = {\n",
    "                \"article\": context,\n",
    "                \"question\": question,\n",
    "                \"options\": options,\n",
    "                \"gold_label\": correct_option_label,\n",
    "                \"article_id\": data['example_id'],\n",
    "                \"title\": \" \",\n",
    "        }\n",
    "        formatted_data[\"train\"].append(output)\n",

    "        \n",
    "    \n",
    "\n",
   
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true,
    "id": "k6mOyvxGDvx-",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chunking Strategies\n",
    "class ChunkingStrategies:\n",
    "    def __init__(self, semantic_model_name=\"all-MiniLM-L6-v2\", provider_name=\"huggingface\"):\n",
    "        # Initialize embedding model for semantic chunking\n",
    "        if provider_name==\"huggingface\":\n",
    "          #self.embeddings = HuggingFaceEmbeddings(model_name= semantic_model_name) \n",
    "           self.embeddings = HuggingFaceBgeEmbeddings(\n",
    "                model_name=semantic_model_name,\n",
    "                model_kwargs={'device': 'cuda'},\n",
    "                encode_kwargs={'normalize_embeddings': True}  # Critical for BGE\n",
    "            )\n",
    "        elif provider_name==\"openai\":\n",
    "          self.embeddings = OpenAIEmbeddings()\n",
    "    def fixed_size_no_overlap(self, text, chunk_size=512):\n",
    "        \"\"\"Fixed-size chunking with no overlap\"\"\"\n",
    "        splitter = TokenTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=0\n",
    "        )\n",
    "        return splitter.split_text(text)\n",
    "\n",
    "    def fixed_size_with_overlap_10(self, text, chunk_size=512):\n",
    "        \"\"\"Fixed-size chunking with 10% overlap\"\"\"\n",
    "        overlap = int(chunk_size * 0.1)\n",
    "        splitter = TokenTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=overlap\n",
    "        )\n",
    "        return splitter.split_text(text)\n",
    "\n",
    "    def fixed_size_with_overlap_20(self, text, chunk_size=512):\n",
    "        \"\"\"Fixed-size chunking with 20% overlap\"\"\"\n",
    "        overlap = int(chunk_size * 0.2)\n",
    "        splitter = TokenTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=overlap\n",
    "        )\n",
    "        return splitter.split_text(text)\n",
    "\n",
    "    def semantic_chunking(self, text):\n",
    "        \"\"\"Semantic chunking using LangChain's SemanticChunker\"\"\"\n",
    "        try:\n",
    "            splitter = SemanticChunker(self.embeddings)\n",
    "            return splitter.split_text(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in semantic chunking: {e}\")\n",
    "            # Fallback to fixed-size chunking if semantic chunking fails\n",
    "            return self.fixed_size_with_overlap_10(text)\n",
    "\n",
    "    def sentence_based_chunking(self, text):\n",
    "        \"\"\"Sentence-based chunking\"\"\"\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \";\", \":\", \" \", \"\"],\n",
    "            chunk_size=512,\n",
    "            chunk_overlap=50\n",
    "        )\n",
    "        return splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qWlvCTRpIHpa"
   },
   "outputs": [],
   "source": [
    "class EmbeddingGenerator:\n",
    "    def __init__(self, model_name, provider):\n",
    "        \"\"\"\n",
    "        Initialize embedding generator using LangChain's interface\n",
    "\n",
    "        Parameters:\n",
    "            model_name (str): Name of the model to use\n",
    "            provider (str): Provider - 'huggingface', 'openai', 'cohere', etc.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.provider = provider\n",
    "        \n",
    "        if provider == \"huggingface\":\n",
    "            self.embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=model_name,\n",
    "                model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "                encode_kwargs={\"normalize_embeddings\": True}\n",
    "            )\n",
    "        elif provider == \"openai\":\n",
    "            self.embeddings = OpenAIEmbeddings(model=model_name)\n",
    "        elif provider == \"cohere\":\n",
    "            self.embeddings = CohereEmbeddings(model=model_name)\n",
    "        elif provider == \"sentence_transformers\":\n",
    "            # For models like Linq-Embed-Mistral that work better with SentenceTransformers\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            self.st_model = SentenceTransformer(model_name)\n",
    "            self.task_instruction = \"Given a query, retrieve passages that answer the query\"\n",
    "            \n",
    "    def generate_embeddings(self, chunks):\n",
    "        \"\"\"Generate embeddings for a list of text chunks\"\"\"\n",
    "        if self.provider == \"sentence_transformers\":\n",
    "            # For document/chunk embeddings with SentenceTransformers\n",
    "            embeddings = self.st_model.encode(chunks, convert_to_numpy=True)\n",
    "            return np.array(embeddings, dtype=np.float32)\n",
    "        else:\n",
    "            # Standard LangChain approach\n",
    "            embeddings = self.embeddings.embed_documents(chunks)\n",
    "            return np.array(embeddings, dtype=np.float32) if isinstance(embeddings, list) else embeddings\n",
    "\n",
    "    def generate_question_embedding(self, question):\n",
    "        \"\"\"Generate embedding for a single question\"\"\"\n",
    "        if self.provider == \"sentence_transformers\":\n",
    "            # For query embeddings with SentenceTransformers, add instruction for models like Linq-Embed-Mistral\n",
    "            prompt = f\"Instruct: {self.task_instruction}\\nQuery: {question}\"\n",
    "            q_embedding = self.st_model.encode(prompt, convert_to_numpy=True)\n",
    "            return np.array(q_embedding, dtype=np.float32)\n",
    "        else:\n",
    "            # Standard LangChain approach\n",
    "            q_embedding = self.embeddings.embed_query(question)\n",
    "            return np.array(q_embedding, dtype=np.float32) if isinstance(q_embedding, list) else q_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lx-HbhGhIQMh"
   },
   "outputs": [],
   "source": [
    "# Vector Database\n",
    "class VectorDatabase:\n",
    "    def __init__(self, dimension):\n",
    "        \"\"\"\n",
    "        Initialize a FAISS vector database with the specified dimension\n",
    "        \"\"\"\n",
    "        self.index = faiss.IndexFlatL2(dimension)\n",
    "        self.texts = []  # Store original texts\n",
    "        self.seen_hashes = set()\n",
    "\n",
    "    def add_embeddings(self, embeddings, texts):\n",
    "        \"\"\"\n",
    "        Add embeddings and their corresponding texts to the database\n",
    "        \"\"\"\n",
    "        if len(embeddings) == 0:\n",
    "            return\n",
    "        unique_emb = []\n",
    "        unique_texts = []\n",
    "        for emb, text in zip(embeddings, texts):\n",
    "            text_hash = sha256(text.encode()).hexdigest()\n",
    "            if text_hash not in self.seen_hashes:\n",
    "                self.seen_hashes.add(text_hash)\n",
    "                unique_emb.append(emb)\n",
    "                unique_texts.append(text)\n",
    "\n",
    "        # Convert to numpy array if it's not already\n",
    "        if isinstance(unique_emb[0], torch.Tensor):\n",
    "            unique_emb = [e.cpu().numpy() for e in unique_emb]\n",
    "\n",
    "        # Add to FAISS index\n",
    "        self.index.add(np.array(unique_emb).astype('float32'))\n",
    "\n",
    "        # Store original texts\n",
    "        self.texts.extend(unique_texts)\n",
    "\n",
    "    def search(self, query_embedding, k=3):\n",
    "        \"\"\"\n",
    "        Search for the k nearest neighbors of the query embedding\n",
    "        Returns distances and indices\n",
    "        \"\"\"\n",
    "        if isinstance(query_embedding, torch.Tensor):\n",
    "            query_embedding = query_embedding.cpu().numpy()\n",
    "\n",
    "        # Reshape if it's a single embedding\n",
    "        if len(query_embedding.shape) == 1:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "\n",
    "        # Search\n",
    "        distances, indices = self.index.search(\n",
    "            np.array(query_embedding).astype('float32'), k\n",
    "        )\n",
    "\n",
    "        # Get the corresponding texts\n",
    "        results = []\n",
    "        for idx_list in indices:\n",
    "            texts = [self.texts[idx] for idx in idx_list if idx < len(self.texts)]\n",
    "            results.append(texts)\n",
    "\n",
    "        return distances, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hhdh4ne-IaLO"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import re\n",
    "\n",
    "class LLMGenerator:\n",
    "    def __init__(self, model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token  # Set pad token\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "    def generate_answer(self, context, question):\n",
    "        # Format the prompt using the correct chat template\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Given the context below, respond with ONLY the letter (A, B, C, or D) of the correct answer.\\n\\nContext: {context}\\n\\nQuestion: {question}\"}\n",
    "        ]\n",
    "        prompt = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Tokenize and get input length\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        input_length = inputs.input_ids.shape[1]  # Get input token count\n",
    "        \n",
    "        # Generate only new tokens\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1,  # Only generate 1 new token\n",
    "                temperature=0.1,\n",
    "                top_p=0.9,\n",
    "                do_sample=True\n",
    "            )\n",
    "        \n",
    "        # Extract ONLY the new tokens (skip input tokens)\n",
    "        new_tokens = outputs[0][input_length:]\n",
    "        response = self.tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "        return response.upper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Xv36B_DEIiay"
   },
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def evaluate_answer(self, generated_answer, gold_answer, question_type=\"multiple_choice\"):\n",
    "        \"\"\"\n",
    "        Evaluate the generated answer against the gold answer\n",
    "        gold_answer should be a letter (A, B, C, D)\n",
    "        \"\"\"\n",
    "        if question_type == \"multiple_choice\":\n",
    "            # Extract the letter choice from the generated answer\n",
    "            match = re.search(r'[A-D]', generated_answer)\n",
    "            if match:\n",
    "                predicted_choice = match.group(0)\n",
    "                correct = predicted_choice == gold_answer\n",
    "            else:\n",
    "                # If no letter is found, count as incorrect\n",
    "                correct = False\n",
    "\n",
    "            return {\n",
    "                \"accuracy\": 1.0 if correct else 0.0\n",
    "            }\n",
    "        else:\n",
    "            # For open-ended questions (not applicable for QuALITY)\n",
    "            return {\n",
    "                \"semantic_similarity\": 0.5  # Placeholder\n",
    "            }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specific_generate_and_save_vector_db(dataset, strategy_name, embedding_model_name, embedding_model_provider,\n",
    "                               semantic_model_name, save_dir, num_samples=100, \n",
    "                                force_rebuild=False):\n",
    "    \"\"\"\n",
    "    Generate a vector database for a specific chunking strategy and save it to disk,\n",
    "    with text-based metrics output.\n",
    "\n",
    "    Parameters:\n",
    "        dataset: The flattened dataset containing documents\n",
    "        strategy_name: Name of the specific chunking strategy to use\n",
    "        embedding_model_name: Name of the embedding model to use\n",
    "        semantic_model_name: Name of semantic model to use for semantic chunking strategy\n",
    "        num_samples: Number of samples to process from training set\n",
    "        save_dir: Directory to save the vector database\n",
    "        force_rebuild: Whether to rebuild database even if it already exists\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with the database path and metrics summary\n",
    "    \"\"\"\n",
    "    print(\"WARNING: This is not the function generate and save db for all strategies, this function will not generate and save eval graphs, it will just rebuild return the metrics for select strategy.\")\n",
    "    # Initialize components\n",
    "    chunking_strategies_obj = ChunkingStrategies(semantic_model_name=semantic_model_name)\n",
    "    embedding_generator = EmbeddingGenerator(model_name = embedding_model_name, provider=embedding_model_provider)\n",
    "    \n",
    "    # Validate the strategy name\n",
    "    chunking_methods = {\n",
    "        \"fixed_size_no_overlap\": chunking_strategies_obj.fixed_size_no_overlap,\n",
    "        \"fixed_size_with_overlap_10\": chunking_strategies_obj.fixed_size_with_overlap_10,\n",
    "        \"fixed_size_with_overlap_20\": chunking_strategies_obj.fixed_size_with_overlap_20,\n",
    "        \"semantic_chunking\": chunking_strategies_obj.semantic_chunking,\n",
    "        \"sentence_based_chunking\": chunking_strategies_obj.sentence_based_chunking\n",
    "    }\n",
    "    \n",
    "    if strategy_name not in chunking_methods:\n",
    "        raise ValueError(f\"Invalid strategy name: {strategy_name}. Available strategies: {list(chunking_methods.keys())}\")\n",
    "    \n",
    "    # Create save directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate file path for the vector database\n",
    "    filename = f\"{strategy_name}_{embedding_model_name.replace('/', '_')}_{num_samples}.pkl\"\n",
    "    db_path = os.path.join(save_dir, filename)\n",
    "    \n",
    "    # Skip if database already exists and force_rebuild is False\n",
    "    if os.path.exists(db_path) and not force_rebuild:\n",
    "        print(f\"Vector database for {strategy_name} already exists at {db_path}. Skipping...\")\n",
    "        return {\"db_path\": db_path}\n",
    "    \n",
    "    print(f\"Building vector database for {strategy_name}...\")\n",
    "    \n",
    "    # Get training samples\n",
    "    train_samples = dataset[\"train\"][:num_samples]\n",
    "    \n",
    "    # Initialize metrics tracking\n",
    "    all_chunks = []\n",
    "    all_embeddings = []\n",
    "    total_chunks = 0\n",
    "    total_chunking_time = 0\n",
    "    total_embedding_time = 0\n",
    "    chunk_sizes = []\n",
    "    chunking_times = []\n",
    "    embedding_times = []\n",
    "    chunks_per_doc = []\n",
    "    \n",
    "    # Process each sample\n",
    "    chunking_method = chunking_methods[strategy_name]\n",
    "    for sample_idx, sample in enumerate(tqdm(train_samples, desc=f\"Processing {strategy_name}\")):\n",
    "        context = sample[\"article\"]\n",
    "        \n",
    "        # Apply chunking\n",
    "        start_time = time.time()\n",
    "        chunks = chunking_method(context)\n",
    "        chunking_time = time.time() - start_time\n",
    "        \n",
    "        # Generate embeddings\n",
    "        start_time = time.time()\n",
    "        chunk_embeddings = embedding_generator.generate_embeddings(chunks)\n",
    "        embedding_time = time.time() - start_time\n",
    "        \n",
    "        # Collect results\n",
    "        all_chunks.extend(chunks)\n",
    "        all_embeddings.extend(chunk_embeddings)\n",
    "        \n",
    "        # Update metrics\n",
    "        chunk_count = len(chunks)\n",
    "        total_chunks += chunk_count\n",
    "        total_chunking_time += chunking_time\n",
    "        total_embedding_time += embedding_time\n",
    "        chunk_sizes.extend([len(chunk) for chunk in chunks])\n",
    "        chunking_times.append(chunking_time)\n",
    "        embedding_times.append(embedding_time)\n",
    "        chunks_per_doc.append(chunk_count)\n",
    "        \n",
    "        # Print progress every 10 samples\n",
    "        if (sample_idx + 1) % 10 == 0:\n",
    "            print(f\"  Processed {sample_idx + 1}/{len(train_samples)} samples\")\n",
    "    \n",
    "    # Create vector database\n",
    "    if len(all_embeddings) > 0:\n",
    "        # Get dimension from first embedding\n",
    "        if isinstance(all_embeddings[0], list):\n",
    "            dimension = len(all_embeddings[0])\n",
    "        else:\n",
    "            dimension = all_embeddings[0].shape[0]\n",
    "        \n",
    "        # Create and populate vector database\n",
    "        vector_db = VectorDatabase(dimension=dimension)\n",
    "        vector_db.add_embeddings(all_embeddings, all_chunks)\n",
    "        \n",
    "        # Save vector database to disk\n",
    "        with open(db_path, 'wb') as f:\n",
    "            pickle.dump(vector_db, f)\n",
    "        \n",
    "        # Calculate and print metrics\n",
    "        avg_chunks_per_doc = total_chunks / len(train_samples)\n",
    "        avg_chunk_size = np.mean(chunk_sizes) if chunk_sizes else 0\n",
    "        avg_chunking_time = np.mean(chunking_times) if chunking_times else 0\n",
    "        avg_embedding_time = np.mean(embedding_times) if embedding_times else 0\n",
    "        time_per_chunk = total_embedding_time / total_chunks if total_chunks > 0 else 0\n",
    "        \n",
    "        # Print metrics summary\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"METRICS SUMMARY FOR {strategy_name}\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Total documents processed: {len(train_samples)}\")\n",
    "        print(f\"Total chunks generated: {total_chunks}\")\n",
    "        print(f\"Average chunks per document: {avg_chunks_per_doc:.2f}\")\n",
    "        print(f\"Average chunk size: {avg_chunk_size:.2f} characters\")\n",
    "        print(f\"Min chunk size: {min(chunk_sizes) if chunk_sizes else 0} characters\")\n",
    "        print(f\"Max chunk size: {max(chunk_sizes) if chunk_sizes else 0} characters\")\n",
    "        print(f\"Total chunking time: {total_chunking_time:.2f} seconds\")\n",
    "        print(f\"Total embedding time: {total_embedding_time:.2f} seconds\")\n",
    "        print(f\"Average chunking time per document: {avg_chunking_time:.4f} seconds\")\n",
    "        print(f\"Average embedding time per document: {avg_embedding_time:.4f} seconds\")\n",
    "        print(f\"Average time per chunk: {time_per_chunk:.4f} seconds\")\n",
    "        print(f\"Vector database saved to: {db_path}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Return metrics as a dictionary\n",
    "        metrics = {\n",
    "            \"db_path\": db_path,\n",
    "            \"total_chunks\": total_chunks,\n",
    "            \"avg_chunks_per_doc\": avg_chunks_per_doc,\n",
    "            \"avg_chunk_size\": avg_chunk_size,\n",
    "            \"min_chunk_size\": min(chunk_sizes) if chunk_sizes else 0,\n",
    "            \"max_chunk_size\": max(chunk_sizes) if chunk_sizes else 0,\n",
    "            \"total_chunking_time\": total_chunking_time,\n",
    "            \"total_embedding_time\": total_embedding_time,\n",
    "            \"avg_chunking_time\": avg_chunking_time,\n",
    "            \"avg_embedding_time\": avg_embedding_time,\n",
    "            \"time_per_chunk\": time_per_chunk\n",
    "        }        \n",
    "        return metrics\n",
    "    else:\n",
    "        print(f\"No embeddings generated for {strategy_name}. Skipping database creation.\")\n",
    "        return {\"db_path\": None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "syij7QVCUxeE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def generate_and_save_vector_dbs(dataset, embedding_model_name, embedding_model_provider, semantic_model_name=\"BAAI/bge-m3\",  chunking_strategies=None,\n",
    "                                 num_samples=100, save_dir=\"vector_dbs\", force_rebuild=False):\n",
    "    \"\"\"\n",
    "    Generate vector databases for each chunking strategy and save them to disk, \n",
    "    along with metrics visualization for performance analysis.\n",
    "\n",
    "    Parameters:\n",
    "        dataset: The flattened dataset containing documents\n",
    "        embedding_model_name: Name of the embedding model to use\n",
    "        semantic_model_name: Name of semantic model to use for semantic chunking strategy\n",
    "        chunking_strategies: List of chunking strategies to evaluate (if None, uses all available)\n",
    "        num_samples: Number of samples to process from training set\n",
    "        save_dir: Directory to save the vector databases\n",
    "        force_rebuild: Whether to rebuild databases even if they already exist\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping strategy names to their saved file paths and metrics plots\n",
    "    \"\"\"\n",
    "    # Initialize components\n",
    "    chunking_strategies_obj = ChunkingStrategies(semantic_model_name = semantic_model_name)\n",
    "    embedding_generator = EmbeddingGenerator(embedding_model_name, embedding_model_provider)\n",
    "    generated_db = False\n",
    "    # Define chunking methods if not provided\n",
    "    if chunking_strategies is None:\n",
    "        chunking_strategies = [\n",
    "            \"fixed_size_no_overlap\",\n",
    "            \"fixed_size_with_overlap_10\",\n",
    "            \"fixed_size_with_overlap_20\",\n",
    "            \"semantic_chunking\",\n",
    "            \"sentence_based_chunking\"\n",
    "        ]\n",
    "\n",
    "    # Create save directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Create metrics directory\n",
    "    metrics_dir = os.path.join(save_dir, \"metrics\")\n",
    "    os.makedirs(metrics_dir, exist_ok=True)\n",
    "\n",
    "    # Mapping between strategy names and methods\n",
    "    chunking_methods = {\n",
    "        \"fixed_size_no_overlap\": chunking_strategies_obj.fixed_size_no_overlap,\n",
    "        \"fixed_size_with_overlap_10\": chunking_strategies_obj.fixed_size_with_overlap_10,\n",
    "        \"fixed_size_with_overlap_20\": chunking_strategies_obj.fixed_size_with_overlap_20,\n",
    "        \"semantic_chunking\": chunking_strategies_obj.semantic_chunking,\n",
    "        \"sentence_based_chunking\": chunking_strategies_obj.sentence_based_chunking\n",
    "    }\n",
    "\n",
    "    # File paths for each vector database\n",
    "    db_paths = {}\n",
    "    for strategy in chunking_strategies:\n",
    "        # Generate a unique filename based on strategy, model, and sample count\n",
    "        filename = f\"{strategy}_{embedding_model_name.replace('/', '_')}_{num_samples}.pkl\"\n",
    "        db_paths[strategy] = os.path.join(save_dir, filename)\n",
    "    \n",
    "    # Initialize metrics tracking data structures\n",
    "    metrics = {\n",
    "        \"strategy\": [],\n",
    "        \"sample_idx\": [],\n",
    "        \"num_chunks\": [],\n",
    "        \"avg_chunk_size\": [],\n",
    "        \"max_chunk_size\": [],\n",
    "        \"min_chunk_size\": [],\n",
    "        \"chunking_time\": [],\n",
    "        \"embedding_time\": [],\n",
    "        \"time_per_chunk\": [],\n",
    "        \"total_processing_time\": []\n",
    "    }\n",
    "    \n",
    "    # Strategy summary metrics\n",
    "    strategy_metrics = defaultdict(lambda: {\n",
    "        \"total_chunks\": 0,\n",
    "        \"total_chunking_time\": 0,\n",
    "        \"total_embedding_time\": 0,\n",
    "        \"num_documents\": 0,\n",
    "        \"chunk_sizes\": [],\n",
    "        \"chunking_times\": [],\n",
    "        \"embedding_times\": [],\n",
    "        \"chunks_per_doc\": []\n",
    "    })\n",
    "\n",
    "    # Generate and save vector databases\n",
    "    for strategy in chunking_strategies:\n",
    "        db_path = db_paths[strategy]\n",
    "\n",
    "        # Skip if database already exists and force_rebuild is False\n",
    "        if os.path.exists(db_path) and not force_rebuild:\n",
    "            print(f\"Vector database for {strategy} already exists at {db_path}. Skipping...\")\n",
    "            continue\n",
    "    \n",
    "        print(f\"Building vector database for {strategy}...\")\n",
    "        generated_db = True\n",
    "        # Get training samples\n",
    "        train_samples = dataset[\"train\"][:num_samples]\n",
    "\n",
    "        # Process each sample\n",
    "        all_chunks = []\n",
    "        all_embeddings = []\n",
    "\n",
    "        # Process metrics\n",
    "        total_chunks = 0\n",
    "        total_chunking_time = 0\n",
    "        total_embedding_time = 0\n",
    "\n",
    "        for sample_idx, sample in enumerate(tqdm(train_samples, desc=f\"Processing {strategy}\")):\n",
    "            context = sample[\"article\"]\n",
    "\n",
    "            # Apply chunking\n",
    "            chunking_method = chunking_methods[strategy]\n",
    "            start_time = time.time()\n",
    "            chunks = chunking_method(context)\n",
    "            chunking_time = time.time() - start_time\n",
    "\n",
    "            # Generate embeddings\n",
    "            start_time = time.time()\n",
    "            chunk_embeddings = embedding_generator.generate_embeddings(chunks)\n",
    "            embedding_time = time.time() - start_time\n",
    "\n",
    "            # Collect results\n",
    "            all_chunks.extend(chunks)\n",
    "            all_embeddings.extend(chunk_embeddings)\n",
    "\n",
    "            # Calculate chunk sizes\n",
    "            chunk_sizes = [len(chunk) for chunk in chunks]\n",
    "            avg_chunk_size = np.mean(chunk_sizes) if chunks else 0\n",
    "            max_chunk_size = max(chunk_sizes) if chunks else 0\n",
    "            min_chunk_size = min(chunk_sizes) if chunks else 0\n",
    "\n",
    "            # Update detailed metrics\n",
    "            metrics[\"strategy\"].append(strategy)\n",
    "            metrics[\"sample_idx\"].append(sample_idx)\n",
    "            metrics[\"num_chunks\"].append(len(chunks))\n",
    "            metrics[\"avg_chunk_size\"].append(avg_chunk_size)\n",
    "            metrics[\"max_chunk_size\"].append(max_chunk_size)\n",
    "            metrics[\"min_chunk_size\"].append(min_chunk_size)\n",
    "            metrics[\"chunking_time\"].append(chunking_time)\n",
    "            metrics[\"embedding_time\"].append(embedding_time)\n",
    "            metrics[\"time_per_chunk\"].append(embedding_time / len(chunks) if chunks else 0)\n",
    "            metrics[\"total_processing_time\"].append(chunking_time + embedding_time)\n",
    "\n",
    "            # Update strategy summary metrics\n",
    "            strategy_metrics[strategy][\"total_chunks\"] += len(chunks)\n",
    "            strategy_metrics[strategy][\"total_chunking_time\"] += chunking_time\n",
    "            strategy_metrics[strategy][\"total_embedding_time\"] += embedding_time\n",
    "            strategy_metrics[strategy][\"num_documents\"] += 1\n",
    "            strategy_metrics[strategy][\"chunk_sizes\"].extend(chunk_sizes)\n",
    "            strategy_metrics[strategy][\"chunking_times\"].append(chunking_time)\n",
    "            strategy_metrics[strategy][\"embedding_times\"].append(embedding_time)\n",
    "            strategy_metrics[strategy][\"chunks_per_doc\"].append(len(chunks))\n",
    "\n",
    "            # Update running totals\n",
    "            total_chunks += len(chunks)\n",
    "            total_chunking_time += chunking_time\n",
    "            total_embedding_time += embedding_time\n",
    "\n",
    "        # Create vector database\n",
    "        if len(all_embeddings) > 0:\n",
    "            # Get dimension from first embedding\n",
    "            if isinstance(all_embeddings[0], list):\n",
    "                dimension = len(all_embeddings[0])\n",
    "            else:\n",
    "                dimension = all_embeddings[0].shape[0]\n",
    "\n",
    "            # Create and populate vector database\n",
    "            vector_db = VectorDatabase(dimension=dimension)\n",
    "            vector_db.add_embeddings(all_embeddings, all_chunks)\n",
    "\n",
    "            # Save vector database to disk\n",
    "            with open(db_path, 'wb') as f:\n",
    "                pickle.dump(vector_db, f)\n",
    "\n",
    "            print(f\"Vector database for {strategy} saved to {db_path}\")\n",
    "            print(f\"  Total chunks: {total_chunks}, Avg chunks per doc: {total_chunks/len(train_samples):.2f}\")\n",
    "            print(f\"  Total chunking time: {total_chunking_time:.2f}s\")\n",
    "            print(f\"  Total embedding time: {total_embedding_time:.2f}s\")\n",
    "        else:\n",
    "            print(f\"No embeddings generated for {strategy}. Skipping database creation.\")\n",
    "    if not generated_db:\n",
    "       return {\n",
    "        \"db_paths\": db_paths,\n",
    "        }\n",
    "    # Convert metrics to DataFrame for easier analysis\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    \n",
    "    # Save detailed metrics to CSV\n",
    "    metrics_csv_path = os.path.join(metrics_dir, f\"chunking_metrics_{embedding_model_name.replace('/', '_')}_{num_samples}.csv\")\n",
    "    metrics_df.to_csv(metrics_csv_path, index=False)\n",
    "    \n",
    "    # Create and save plots\n",
    "    plot_paths = {}\n",
    "    \n",
    "    # 1. Average Chunking Time by Strategy\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    strategies = list(strategy_metrics.keys())\n",
    "    avg_chunking_times = [np.mean(strategy_metrics[s][\"chunking_times\"]) for s in strategies]\n",
    "    \n",
    "    plt.bar(strategies, avg_chunking_times)\n",
    "    plt.title('Average Chunking Time by Strategy')\n",
    "    plt.xlabel('Chunking Strategy')\n",
    "    plt.ylabel('Average Time (seconds)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    chunking_time_plot = os.path.join(metrics_dir, f\"chunking_time_{embedding_model_name.replace('/', '_')}_{num_samples}.png\")\n",
    "    plt.savefig(chunking_time_plot)\n",
    "    plot_paths[\"chunking_time\"] = chunking_time_plot\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Average Embedding Time by Strategy\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    avg_embedding_times = [np.mean(strategy_metrics[s][\"embedding_times\"]) for s in strategies]\n",
    "    \n",
    "    plt.bar(strategies, avg_embedding_times)\n",
    "    plt.title('Average Embedding Time by Strategy')\n",
    "    plt.xlabel('Chunking Strategy')\n",
    "    plt.ylabel('Average Time (seconds)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    embedding_time_plot = os.path.join(metrics_dir, f\"embedding_time_{embedding_model_name.replace('/', '_')}_{num_samples}.png\")\n",
    "    plt.savefig(embedding_time_plot)\n",
    "    plot_paths[\"embedding_time\"] = embedding_time_plot\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Average Number of Chunks per Document by Strategy\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    avg_chunks_per_doc = [np.mean(strategy_metrics[s][\"chunks_per_doc\"]) for s in strategies]\n",
    "    \n",
    "    plt.bar(strategies, avg_chunks_per_doc)\n",
    "    plt.title('Average Number of Chunks per Document by Strategy')\n",
    "    plt.xlabel('Chunking Strategy')\n",
    "    plt.ylabel('Average Number of Chunks')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    chunks_plot = os.path.join(metrics_dir, f\"chunks_per_doc_{embedding_model_name.replace('/', '_')}_{num_samples}.png\")\n",
    "    plt.savefig(chunks_plot)\n",
    "    plot_paths[\"chunks_per_doc\"] = chunks_plot\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Average Chunk Size by Strategy\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    avg_chunk_sizes = [np.mean(strategy_metrics[s][\"chunk_sizes\"]) for s in strategies]\n",
    "    \n",
    "    plt.bar(strategies, avg_chunk_sizes)\n",
    "    plt.title('Average Chunk Size by Strategy')\n",
    "    plt.xlabel('Chunking Strategy')\n",
    "    plt.ylabel('Average Chunk Size (characters)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    chunk_size_plot = os.path.join(metrics_dir, f\"chunk_size_{embedding_model_name.replace('/', '_')}_{num_samples}.png\")\n",
    "    plt.savefig(chunk_size_plot)\n",
    "    plot_paths[\"chunk_size\"] = chunk_size_plot\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Chunking Time vs. Number of Chunks Scatter Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        plt.scatter(\n",
    "            strategy_metrics[strategy][\"chunks_per_doc\"],\n",
    "            strategy_metrics[strategy][\"chunking_times\"],\n",
    "            label=strategy,\n",
    "            alpha=0.7\n",
    "        )\n",
    "    \n",
    "    plt.title('Chunking Time vs Number of Chunks')\n",
    "    plt.xlabel('Number of Chunks')\n",
    "    plt.ylabel('Chunking Time (seconds)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    scatter_plot = os.path.join(metrics_dir, f\"chunking_vs_chunks_{embedding_model_name.replace('/', '_')}_{num_samples}.png\")\n",
    "    plt.savefig(scatter_plot)\n",
    "    plot_paths[\"chunking_vs_chunks\"] = scatter_plot\n",
    "    plt.close()\n",
    "    \n",
    "    # 6. Processing Time Distribution (Stacked Bar)\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    x = np.arange(len(strategies))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Calculate average times for each strategy\n",
    "    avg_chunking = [np.mean(strategy_metrics[s][\"chunking_times\"]) for s in strategies]\n",
    "    avg_embedding = [np.mean(strategy_metrics[s][\"embedding_times\"]) for s in strategies]\n",
    "    \n",
    "    # Create stacked bar chart\n",
    "    plt.bar(x, avg_chunking, width, label='Chunking Time')\n",
    "    plt.bar(x, avg_embedding, width, bottom=avg_chunking, label='Embedding Time')\n",
    "    \n",
    "    plt.title('Average Processing Time Distribution by Strategy')\n",
    "    plt.xlabel('Chunking Strategy')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.xticks(x, strategies, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    time_distribution_plot = os.path.join(metrics_dir, f\"time_distribution_{embedding_model_name.replace('/', '_')}_{num_samples}.png\")\n",
    "    plt.savefig(time_distribution_plot)\n",
    "    plot_paths[\"time_distribution\"] = time_distribution_plot\n",
    "    plt.close()\n",
    "    \n",
    "    # 7. Chunk Size Distribution (Box Plot)\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    chunk_size_data = [strategy_metrics[s][\"chunk_sizes\"] for s in strategies]\n",
    "    plt.boxplot(chunk_size_data, labels=strategies)\n",
    "    \n",
    "    plt.title('Chunk Size Distribution by Strategy')\n",
    "    plt.xlabel('Chunking Strategy')\n",
    "    plt.ylabel('Chunk Size (characters)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    size_distribution_plot = os.path.join(metrics_dir, f\"size_distribution_{embedding_model_name.replace('/', '_')}_{num_samples}.png\")\n",
    "    plt.savefig(size_distribution_plot)\n",
    "    plot_paths[\"size_distribution\"] = size_distribution_plot\n",
    "    plt.close()\n",
    "    \n",
    "    # Create a summary plot dashboard\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    \n",
    "    # 1. Top left: Avg chunks per document\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.bar(strategies, avg_chunks_per_doc)\n",
    "    plt.title('Average Chunks per Document')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # 2. Top right: Avg chunk size\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.bar(strategies, avg_chunk_sizes)\n",
    "    plt.title('Average Chunk Size')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # 3. Bottom left: Processing time breakdown\n",
    "    plt.subplot(2, 2, 3)\n",
    "    x = np.arange(len(strategies))\n",
    "    plt.bar(x, avg_chunking, width, label='Chunking Time')\n",
    "    plt.bar(x, avg_embedding, width, bottom=avg_chunking, label='Embedding Time')\n",
    "    plt.title('Processing Time Breakdown')\n",
    "    plt.xticks(x, strategies, rotation=45)\n",
    "    plt.legend()\n",
    "    \n",
    "    # 4. Bottom right: Time per chunk\n",
    "    plt.subplot(2, 2, 4)\n",
    "    time_per_chunk = [np.sum(strategy_metrics[s][\"embedding_times\"])/strategy_metrics[s][\"total_chunks\"] \n",
    "                      if strategy_metrics[s][\"total_chunks\"] > 0 else 0 \n",
    "                      for s in strategies]\n",
    "    plt.bar(strategies, time_per_chunk)\n",
    "    plt.title('Average Time per Chunk')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    dashboard_plot = os.path.join(metrics_dir, f\"metrics_dashboard_{embedding_model_name.replace('/', '_')}_{num_samples}.png\")\n",
    "    plt.savefig(dashboard_plot)\n",
    "    plot_paths[\"dashboard\"] = dashboard_plot\n",
    "    plt.close()\n",
    "    \n",
    "    # Return both database paths and plot paths\n",
    "    return {\n",
    "        \"db_paths\": db_paths,\n",
    "        \"plot_paths\": plot_paths,\n",
    "        \"metrics_csv\": metrics_csv_path\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fIJyDRDaVauA"
   },
   "outputs": [],
   "source": [
    "def load_vector_dbs(db_paths):\n",
    "    \"\"\"\n",
    "    Load vector databases from disk\n",
    "\n",
    "    Parameters:\n",
    "        db_paths: Dictionary mapping strategy names to file paths\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping strategy names to loaded vector database objects\n",
    "    \"\"\"\n",
    "    vector_dbs = {}\n",
    "\n",
    "    for strategy, path in db_paths.items():\n",
    "        if os.path.exists(path):\n",
    "            print(f\"Loading vector database for {strategy} from {path}\")\n",
    "            try:\n",
    "                with open(path, 'rb') as f:\n",
    "                    vector_dbs[strategy] = pickle.load(f)\n",
    "                print(f\"  Successfully loaded vector database with {len(vector_dbs[strategy].texts)} chunks\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error loading database: {e}\")\n",
    "        else:\n",
    "            print(f\"Vector database for {strategy} not found at {path}\")\n",
    "\n",
    "    return vector_dbs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_single_strategy(dataset, strategy_name,  \n",
    "                                   question_embedding_model_name, \n",
    "                                   question_embedding_model_provider,\n",
    "                                   db_path=None, \n",
    "                                  num_samples=100, top_k=5, include_no_context=False):\n",
    "    \"\"\"\n",
    "    Run the experiment using a single pre-built vector database\n",
    "\n",
    "    Parameters:\n",
    "        dataset: The flattened dataset containing documents and questions\n",
    "        strategy_name: Name of the chunking strategy to evaluate\n",
    "        db_path: Path to the vector database file (if None, skips RAG)\n",
    "        question_embedding_model_name: Name of the embedding model to use for questions\n",
    "        num_samples: Number of validation samples to process\n",
    "        top_k: Number of chunks to retrieve\n",
    "        include_no_context: Whether to also run a no-context baseline\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with experiment results for the specified chunking strategy\n",
    "    \"\"\"\n",
    "    # Initialize components\n",
    "    embedding_generator = EmbeddingGenerator(question_embedding_model_name,question_embedding_model_provider)\n",
    "    llm_generator = LLMGenerator()\n",
    "    evaluator = Evaluator()\n",
    "\n",
    "    # Results storage\n",
    "    results = defaultdict(list)\n",
    "    \n",
    "    # Strategies to evaluate\n",
    "    strategies_to_run = []\n",
    "    if strategy_name != \"no_context\":\n",
    "        strategies_to_run.append(strategy_name)\n",
    "    if include_no_context:\n",
    "        strategies_to_run.append(\"no_context\")\n",
    "    \n",
    "    # Load vector database if path is provided\n",
    "    vector_db = None\n",
    "    if db_path is not None and os.path.exists(db_path) and strategy_name != \"no_context\":\n",
    "        print(f\"Loading vector database from {db_path}\")\n",
    "        try:\n",
    "            with open(db_path, 'rb') as f:\n",
    "                vector_db = pickle.load(f)\n",
    "            print(f\"Successfully loaded vector database with {len(vector_db.texts)} chunks\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading database: {e}\")\n",
    "            return results\n",
    "    elif strategy_name != \"no_context\":\n",
    "        print(f\"Vector database path not provided or does not exist for {strategy_name}. Skipping RAG...\")\n",
    "        return results\n",
    "        \n",
    "    validation_samples = dataset[\"train\"][:num_samples]\n",
    "    print(f\"Running experiment on {len(validation_samples)} validation samples with strategy: {strategy_name}\")\n",
    "    if include_no_context:\n",
    "        print(\"Also running no-context baseline for comparison\")\n",
    "\n",
    "    # Track overall metrics\n",
    "    total_correct = 0\n",
    "    total_retrieval_time = 0\n",
    "    total_generation_time = 0\n",
    "    no_context_correct = 0 if include_no_context else None\n",
    "    \n",
    "    # Process each sample\n",
    "    for sample_idx, sample in enumerate(tqdm(validation_samples, desc=\"Processing samples\")):\n",
    "        # Extract data from the flattened sample\n",
    "        context = sample[\"article\"]\n",
    "        question = sample[\"question\"]\n",
    "        options = sample[\"options\"]\n",
    "        gold_answer = sample[\"gold_label\"]  # Should be A, B, C, or D\n",
    "\n",
    "        # Format the question with choices\n",
    "        formatted_question = f\"{question}\\n\"\n",
    "        for i, option_text in enumerate(options):\n",
    "            choice_letter = chr(65 + i)  # A, B, C, D...\n",
    "            formatted_question += f\"{choice_letter}. {option_text}\\n\"\n",
    "\n",
    "        # Process each strategy\n",
    "        for strategy in strategies_to_run:\n",
    "            retrieval_time = 0\n",
    "            retrieved_chunks = []\n",
    "            \n",
    "            print(f\"\\nSample {sample_idx+1}/{len(validation_samples)}, Method: {strategy}\")\n",
    "            \n",
    "            if strategy == \"no_context\":\n",
    "                # Empty context for baseline\n",
    "                combined_context = \"Assume context is not available\"\n",
    "            else:\n",
    "                # Generate question embedding\n",
    "                question_embedding = embedding_generator.generate_question_embedding(formatted_question)\n",
    "    \n",
    "                # Retrieve relevant chunks from the pre-built vector database\n",
    "                start_time = time.time()\n",
    "                _, retrieved_chunks = vector_db.search(question_embedding, k=top_k)\n",
    "                retrieval_time = time.time() - start_time\n",
    "                \n",
    "                # Ensure retrieved_chunks is properly formatted\n",
    "                if isinstance(retrieved_chunks[0], list):\n",
    "                    retrieved_chunks = retrieved_chunks[0]\n",
    "    \n",
    "                # Combine retrieved chunks into a single context\n",
    "                combined_context = \"\\n\\n\".join(retrieved_chunks)\n",
    "    \n",
    "            # Generate answer using LLM\n",
    "            start_time = time.time()\n",
    "            generated_answer = llm_generator.generate_answer(combined_context, formatted_question)\n",
    "            generation_time = time.time() - start_time\n",
    "    \n",
    "            # Evaluate answer\n",
    "            answer_metrics = evaluator.evaluate_answer(generated_answer, gold_answer)\n",
    "            combined_context_separated = \"\\nNEW_CHUNK\\n\".join(retrieved_chunks) if retrieved_chunks else \"\"\n",
    "            \n",
    "            # Update metrics\n",
    "            if strategy == \"no_context\":\n",
    "                no_context_correct += answer_metrics[\"accuracy\"]\n",
    "            else:\n",
    "                total_correct += answer_metrics[\"accuracy\"]\n",
    "                total_retrieval_time += retrieval_time\n",
    "                total_generation_time += generation_time\n",
    "            \n",
    "            # Print detailed output for this sample\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Gold Answer: {gold_answer}\")\n",
    "            print(f\"Generated Answer: {generated_answer}\")\n",
    "            print(f\"Correct: {'Yes' if answer_metrics['accuracy'] == 1 else 'No'}\")\n",
    "            if strategy != \"no_context\":\n",
    "                print(f\"Retrieval Time: {retrieval_time:.4f}s\")\n",
    "            print(f\"Generation Time: {generation_time:.4f}s\")\n",
    "            if strategy != \"no_context\":\n",
    "                print(f\"Retrieved {len(retrieved_chunks)} chunks\")\n",
    "            \n",
    "            # Store results\n",
    "            results[strategy].append({\n",
    "                \"sample_idx\": sample_idx,\n",
    "                \"retrieval_time\": retrieval_time,\n",
    "                \"generation_time\": generation_time,\n",
    "                \"answer_accuracy\": answer_metrics[\"accuracy\"],\n",
    "                \"retrieved_context\": combined_context_separated,\n",
    "                \"question\": formatted_question,\n",
    "                \"generated_answer\": generated_answer,\n",
    "                \"gold_answer\": gold_answer,\n",
    "                \"article_id\": sample[\"article_id\"],\n",
    "                \"title\": sample[\"title\"]\n",
    "            })\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"EXPERIMENT SUMMARY FOR {strategy_name}\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total samples: {len(validation_samples)}\")\n",
    "    \n",
    "    if strategy_name != \"no_context\":\n",
    "        accuracy = total_correct / len(validation_samples) if len(validation_samples) > 0 else 0\n",
    "        avg_retrieval_time = total_retrieval_time / len(validation_samples) if len(validation_samples) > 0 else 0\n",
    "        avg_generation_time = total_generation_time / len(validation_samples) if len(validation_samples) > 0 else 0\n",
    "        \n",
    "        print(f\"Strategy: {strategy_name}\")\n",
    "        print(f\"Accuracy: {accuracy:.2%} ({total_correct}/{len(validation_samples)})\")\n",
    "        print(f\"Average Retrieval Time: {avg_retrieval_time:.4f}s\")\n",
    "        print(f\"Average Generation Time: {avg_generation_time:.4f}s\")\n",
    "        print(f\"Average Total Time: {(avg_retrieval_time + avg_generation_time):.4f}s\")\n",
    "    \n",
    "    if include_no_context:\n",
    "        no_context_accuracy = no_context_correct / len(validation_samples) if len(validation_samples) > 0 else 0\n",
    "        print(f\"\\nNo-Context Baseline:\")\n",
    "        print(f\"Accuracy: {no_context_accuracy:.2%} ({no_context_correct}/{len(validation_samples)})\")\n",
    "        \n",
    "        if strategy_name != \"no_context\":\n",
    "            improvement = accuracy - no_context_accuracy\n",
    "            print(f\"\\nImprovement over No-Context: {improvement:.2%}\")\n",
    "            if improvement > 0:\n",
    "                print(f\"RAG with {strategy_name} improved accuracy by {improvement:.2%}\")\n",
    "            elif improvement < 0:\n",
    "                print(f\"RAG with {strategy_name} decreased accuracy by {abs(improvement):.2%}\")\n",
    "            else:\n",
    "                print(f\"RAG with {strategy_name} had no effect on accuracy\")\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-SCyBlzeIp2-"
   },
   "outputs": [],
   "source": [
    "def run_experiment(dataset, question_embedding_model_name, question_embedding_model_provider, db_paths=None,  num_samples=20, top_k=5):\n",
    "    \"\"\"\n",
    "    Run the experiment using pre-built vector databases\n",
    "\n",
    "    Parameters:\n",
    "        dataset: The flattened dataset containing documents and questions\n",
    "        db_paths: Dictionary mapping strategy names to file paths (if None, generates temp DBs)\n",
    "        model_name: Name of the embedding model to use\n",
    "        num_samples: Number of validation samples to process\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with experiment results for each chunking strategy\n",
    "    \"\"\"\n",
    "    # Initialize components\n",
    "    embedding_generator = EmbeddingGenerator(question_embedding_model_name, question_embedding_model_provider)\n",
    "    llm_generator = LLMGenerator()\n",
    "    evaluator = Evaluator()\n",
    "\n",
    "    # Results storage\n",
    "    results = defaultdict(list)\n",
    "\n",
    "    # Load vector databases if paths are provided\n",
    "    if db_paths is None:\n",
    "      print(\"NO PATHS PROVIDED! early return\")\n",
    "      return results  # Return empty results if no paths provided\n",
    "    vector_dbs = load_vector_dbs(db_paths)\n",
    "    chunking_strategies = list(vector_dbs.keys())\n",
    "    chunking_strategies.append(\"no_context\") # to eval baseline accuracy without RAG\n",
    "    validation_samples = dataset[\"train\"][:num_samples]\n",
    "    print(f\"Running experiment on {len(validation_samples)} validation samples with {len(chunking_strategies)} chunking strategies (including no-context baseline)\")\n",
    "\n",
    "    # Process each sample\n",
    "    for sample_idx, sample in enumerate(tqdm(validation_samples, desc=\"Processing samples\")):\n",
    "        # Extract data from the flattened sample\n",
    "        context = sample[\"article\"]\n",
    "        question = sample[\"question\"]\n",
    "        options = sample[\"options\"]\n",
    "        gold_answer = sample[\"gold_label\"]  # Should be A, B, C, or D\n",
    "\n",
    "        # Format the question with choices\n",
    "        formatted_question = f\"{question}\\n\"\n",
    "        for i, option_text in enumerate(options):\n",
    "            choice_letter = chr(65 + i)  # A, B, C, D...\n",
    "            formatted_question += f\"{choice_letter}. {option_text}\\n\"\n",
    "\n",
    "        # Process with each chunking method's vector database\n",
    "        for strategy in chunking_strategies:\n",
    "            retrieval_time = 0\n",
    "            retrieved_chunks = []\n",
    "            if strategy not in vector_dbs and strategy!=\"no_context\":\n",
    "                print(f\"Vector database for {strategy} not found. Skipping...\")\n",
    "                continue\n",
    "            print(f\"\\nSample {sample_idx+1}/{len(validation_samples)}, Method: {strategy}\")\n",
    "            if strategy == \"no_context\":\n",
    "                # Empty context for baseline\n",
    "                combined_context = \"Assume context is not available\"\n",
    "            else:\n",
    "                # Generate question embedding\n",
    "                question_embedding = embedding_generator.generate_question_embedding(formatted_question)\n",
    "    \n",
    "                # Retrieve relevant chunks from the pre-built vector database\n",
    "                start_time = time.time()\n",
    "                distances, retrieved_chunks = vector_dbs[strategy].search(question_embedding, k=top_k)\n",
    "                retrieval_time = time.time() - start_time\n",
    "                \n",
    "                # Ensure retrieved_chunks is properly formatted\n",
    "                if isinstance(retrieved_chunks[0], list):\n",
    "                    retrieved_chunks = retrieved_chunks[0]\n",
    "    \n",
    "                # Combine retrieved chunks into a single context\n",
    "                combined_context = \"\\n\\n\".join(retrieved_chunks)\n",
    "    \n",
    "            # Generate answer using LLM\n",
    "            start_time = time.time()\n",
    "            generated_answer = llm_generator.generate_answer(combined_context, formatted_question)\n",
    "            generation_time = time.time() - start_time\n",
    "    \n",
    "            # Evaluate answer\n",
    "            answer_metrics = evaluator.evaluate_answer(generated_answer, gold_answer)\n",
    "            combined_context_separated = \"\\nNEW_CHUNK\\n\".join(retrieved_chunks) if retrieved_chunks else \"\"\n",
    "            # Store results\n",
    "            results[strategy].append({\n",
    "                \"sample_idx\": sample_idx,\n",
    "                \"retrieval_time\": retrieval_time,\n",
    "                \"generation_time\": generation_time,\n",
    "                \"answer_accuracy\": answer_metrics[\"accuracy\"],\n",
    "                \"distances\": distances,\n",
    "                \"retrieved_context\": combined_context_separated,\n",
    "                \"question\": formatted_question,\n",
    "                \"generated_answer\": generated_answer,\n",
    "                \"gold_answer\": gold_answer,\n",
    "                \"article_id\": sample[\"article_id\"],\n",
    "                \"title\": sample[\"title\"]\n",
    "            })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "4uU86JFMKWBC"
   },
   "outputs": [],
   "source": [
    "def analyze_results(results):\n",
    "    \"\"\"\n",
    "    Analyze and visualize the experimental results\n",
    "    \"\"\"\n",
    "    # Convert results to DataFrame for easier analysis\n",
    "    all_data = []\n",
    "    for method, method_results in results.items():\n",
    "        for result in method_results:\n",
    "            result_copy = result.copy()\n",
    "            result_copy[\"method\"] = method\n",
    "            all_data.append(result_copy)\n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "\n",
    "    # Calculate aggregate metrics 'chunking_time', 'contains_answer', 'embedding_time', 'num_chunks', 'retrieval_precision'\n",
    "    aggregates = df.groupby(\"method\").agg({\n",
    "        \"retrieval_time\": \"mean\",\n",
    "        \"generation_time\": \"mean\",\n",
    "        \"answer_accuracy\": \"mean\"\n",
    "    }).reset_index()\n",
    "\n",
    "    print(\"\\n=== Aggregate Results ===\")\n",
    "    print(aggregates.to_string(index=False))\n",
    "\n",
    "    # Save detailed results to CSV\n",
    "    df.to_csv(\"chunking_experiment_results.csv\", index=False)\n",
    "\n",
    "    # Create visualizations\n",
    "    # Plot accuracy by chunking method\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(aggregates[\"method\"], aggregates[\"answer_accuracy\"])\n",
    "    plt.title(\"Answer Accuracy by Chunking Method\")\n",
    "    plt.xlabel(\"Chunking Method\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"accuracy_by_method.png\")\n",
    "\n",
    "    # Plot processing times by chunking method\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    # Create a grouped bar chart for different time metrics\n",
    "    bar_width = 0.15\n",
    "    index = np.arange(len(aggregates[\"method\"]))\n",
    "    plt.bar(index + 0.5*bar_width, aggregates[\"retrieval_time\"], bar_width, label=\"Retrieval Time\")\n",
    "    plt.bar(index + 1.5*bar_width, aggregates[\"generation_time\"], bar_width, label=\"Generation Time\")\n",
    "    plt.xlabel(\"Chunking Method\")\n",
    "    plt.ylabel(\"Time (seconds)\")\n",
    "    plt.title(\"Processing Times by Chunking Method\")\n",
    "    plt.xticks(index, aggregates[\"method\"], rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"processing_times.png\")\n",
    "\n",
    "    return df, aggregates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnable Code begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3kVjtW5Wq9d"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# Step 1: Load and flatten the QuALITY dataset\n",
    "dataset = load_quality_github_dataset()\n",
    "dataset = flatten_quality_dataset(dataset)\n",
    "\n",
    "# Step 1:Load the wikiQA dataset\n",
    "# dataset = load_wikiqa_dataset()\n",
    "\n",
    "# Step 1:Load the squad dataset\n",
    "#dataset = load_squad()\n",
    "\n",
    "    \n",
    "# Step 1:Load the NQ dataset\n",
    "# dataset = load_natural_questions()"

   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose a model, set its name and provider\n",
    "embedding_model_name = \"Linq-AI-Research/Linq-Embed-Mistral\"\n",
    "#embedding_model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "embedding_model_provider = \"sentence_transformers\"\n",
    "semantic_model_name = \"all-MiniLM-L6-v2\"\n",
    "save_dir_name = \"vectorDB_linq_miniLM_quality_dataset_300\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "4JJtRvcIWw-O",
    "outputId": "376039f4-25fd-4db7-f21c-035f34b4eb78"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1087/1990588150.py:7: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embeddings = HuggingFaceBgeEmbeddings(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db0005e0687452c8b44b001aec79ef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 2: Generate and save vector databases (do this once, takes time)\n",
    "db_paths = generate_and_save_vector_dbs(\n",
    "    dataset,\n",
    "    embedding_model_name = embedding_model_name, #the embedding model used for all the chunking strategies\n",
    "    embedding_model_provider = embedding_model_provider, # the provider for the model eg. huggingface, SentenceTransformer\n",
    "    semantic_model_name= semantic_model_name, #change to use a specific model for semantic chunking\n",
    "    num_samples=300,\n",
    "    save_dir=save_dir_name, #the name should cover all the choices used to build it\n",
    "    force_rebuild=False # Set to True to rebuild even if already exists\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(db_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "XpeHqU-wKalq",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 3: Run experiments using the pre-built databases\n",
    "results = run_experiment(\n",
    "    dataset,\n",
    "    db_paths=db_paths[\"db_paths\"],\n",
    "    question_embedding_model_name=embedding_model_name, #model used just to generate the question embeddings\n",
    "    question_embedding_model_provider = embedding_model_provider, \n",
    "    num_samples=200,  # Number of samples to run inference on\n",
    "    top_k=5 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "CIwFiKzeW-RW",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Step 4: Analyze and visualize results\n",
    "df, aggregates = analyze_results(results)\n",
    "print(\"\\nExperiment completed! Results saved to chunking_experiment_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
